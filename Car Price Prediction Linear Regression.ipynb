{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87830b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d43bc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Usrs\\\\user\\\\Downloads\\\\CarPrice_Assignment.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8c6bda948c63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Read in the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Usrs\\\\user\\\\Downloads\\\\CarPrice_Assignment.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Usrs\\\\user\\\\Downloads\\\\CarPrice_Assignment.csv'"
     ]
    }
   ],
   "source": [
    "#Read in the data\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\user\\\\Downloads\\\\CarPrice_Assignment.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b49a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215dec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_ID is  randomly generated thereby not a predictive feature so we drop it\n",
    "df = df.drop(columns = 'car_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47278964",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7089188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#symboling is a categorical column so lets convert it \n",
    "df['symboling'] = df['symboling'].astype('object')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4073187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the categorical and numerical columns\n",
    "categorical_columns = df.select_dtypes(\"object\")\n",
    "numerical_columns = df.select_dtypes(['float64','int64',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db07031",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66531c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692257f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets observe how the numerical data is distributed\n",
    "for columns in numerical_columns.columns:\n",
    "    sns.distplot(df[columns])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b41849",
   "metadata": {},
   "outputs": [],
   "source": [
    "for columns in numerical_columns.columns:\n",
    "    sns.scatterplot(x = df[columns] ,y = df['price'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2199342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observe the correlation between the features\n",
    "plt.figure(figsize = (16,8))\n",
    "sns.heatmap(numerical_columns.corr(),cmap = 'YlGnBu',annot = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets observe the unique items in each categorical features\n",
    "for columns in categorical_columns.columns:\n",
    "    print(df[columns].value_counts(),'\\n\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1148c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the carName column\n",
    "df['CarName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4459874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Car Brand'] = df['CarName'].apply(lambda x:x.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79783b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Car Brand'].value_counts() # This look cleaner but at the end of the list,you will observe what might have been typos so we clean further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd7e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Car Brand'].loc[(df['Car Brand'] == 'vokswagen') |(df['Car Brand'] == 'vw')] = 'volkswagen'\n",
    "\n",
    "df['Car Brand'].loc[df['Car Brand'] == 'porcshce'] = 'porsche'\n",
    "\n",
    "df['Car Brand'].loc[df['Car Brand'] == 'toyouta'] = 'toyota'\n",
    "\n",
    "df['Car Brand'].loc[df['Car Brand'] == 'Nissan'] = 'nissan'\n",
    "\n",
    "df['Car Brand'].loc[df['Car Brand'] == 'maxda'] = 'mazda'\n",
    "\n",
    "df['Car Brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns.drop(columns = 'CarName',inplace = True)\n",
    "categorical_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Car Brand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab856423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the Car Brand Column to our categorical_column after dropping the Car Name column\n",
    "categorical_columns = pd.merge(df['Car Brand'],categorical_columns,left_index = True,right_index = True)\n",
    "categorical_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a335022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drp car Name column from df\n",
    "df.drop(columns = 'CarName',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43580b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e48deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the categorical columns,lets get dummies but drop those columns\n",
    "df = pd.get_dummies(df,columns = categorical_columns.columns,drop_first = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a893f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the dependent variable from independent variables\n",
    "X = df.drop(columns = 'price')\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb273af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Scale our feautures to make coefficient interpretation easier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X = pd.DataFrame(X_scaled,columns =X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be97369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train your model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions from your model\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate your model\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "train_set_accuracy = r2_score(y_true = y_train,y_pred = train_predictions)\n",
    "test_set_accuracy = r2_score(y_true = y_test,y_pred = test_predictions)\n",
    "\n",
    "print(\"Training Set R2 Score:\",train_set_accuracy)\n",
    "print(\"Test Set R2 Score:\",test_set_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"MSE for test set:\",mean_squared_error(y_test,test_predictions))\n",
    "print(\"MSE for training set:\",mean_squared_error(y_train,train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7360bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From our scores,we can see that our model is overfitting.Let us try to reduce the overfit by selecting optimal features from\n",
    "# our many features and train the model with those features then pick one that results in better r2-scores but does not overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "train_r2 = []\n",
    "test_r2 = []\n",
    "train_MSE = []\n",
    "test_MSE = []\n",
    "\n",
    "for n_features in range(4,31):\n",
    "    lg = LinearRegression()\n",
    "    \n",
    "    rfe = RFE(estimator = lg,n_features_to_select = n_features)\n",
    "    \n",
    "    rfe.fit(X_train,y_train)\n",
    "    \n",
    "    selected_features = X_train.columns[rfe.support_]\n",
    "    \n",
    "    X_train_rfe = X_train[selected_features]\n",
    "    X_test_rfe = X_test[selected_features]\n",
    "    \n",
    "    #add a constant to the model\n",
    "    X_train_rfe = sm.add_constant(X_train_rfe,has_constant = 'add')\n",
    "    X_test_rfe = sm.add_constant(X_test_rfe,has_constant = 'add')\n",
    "    \n",
    "    #fit the model with the selected feaures\n",
    "    rfe_model = sm.OLS(y_train,X_train_rfe).fit()\n",
    "    \n",
    "    #Make predictions\n",
    "    test_predictions_rfe = rfe_model.predict(X_test_rfe)\n",
    "    train_predictions_rfe = rfe_model.predict(X_train_rfe)\n",
    "    \n",
    "    #Evaluate your model\n",
    "    train_r2.append(rfe_model.rsquared)\n",
    "    test_r2.append(r2_score(y_test,test_predictions_rfe))\n",
    "    \n",
    "    train_MSE.append(mean_squared_error(train_predictions_rfe,y_train))\n",
    "    test_MSE.append(mean_squared_error(test_predictions_rfe,y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting R2 Score Against the number of features\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(range(4,31),train_r2,'r',label = 'R2 Train')\n",
    "plt.plot(range(4,31),test_r2,'b',label = 'R2 Test')\n",
    "plt.xticks(range(2,35))\n",
    "plt.xlabel(\"No. of Features\")\n",
    "plt.ylabel(\"R2 Scores\")\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7bdff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Mean Squared error\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(range(4,31),train_MSE,'r',label = 'MSE Train')\n",
    "plt.plot(range(4,31),test_MSE,'b',label = 'MSE Test')\n",
    "plt.xticks(range(2,35))\n",
    "plt.xlabel(\"No. of Features\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11-13 features might be our best choice with no high bias and no high variance\n",
    "#Lets make a model with 12 features\n",
    "lg = LinearRegression()\n",
    "\n",
    "rfe = RFE(estimator = lg,n_features_to_select = 13)\n",
    "rfe.fit(X_train,y_train)\n",
    "\n",
    "selected_columns = X_train.columns[rfe.support_]\n",
    "\n",
    "X_train_rfe = X_train[selected_columns]\n",
    "X_test_rfe = X_test[selected_columns]\n",
    "\n",
    "X_train_rfe = sm.add_constant(X_train_rfe,has_constant = 'add')\n",
    "X_test_rfe = sm.add_constant(X_test_rfe,has_constant = 'add')\n",
    "    \n",
    "rfe_model = sm.OLS(y_train,X_train_rfe).fit()\n",
    "\n",
    "y_pred_train = rfe_model.predict(X_train_rfe)\n",
    "y_pred_test = rfe_model.predict(X_test_rfe)\n",
    "    \n",
    "train_r2 = rfe_model.rsquared\n",
    "test_r2 = r2_score(y_test,y_pred_test)\n",
    "    \n",
    "error_test=y_pred_test-y_test\n",
    "error_train=y_pred_train-y_train\n",
    "    \n",
    "test_RMSE=(((error_test**2).mean())**0.5)\n",
    "train_RMSE=(((error_train**2).mean())**0.5)\n",
    "    \n",
    "print(\"......................................R2_Score............................................\")\n",
    "print(\"Training data r2 Score:\",train_r2)\n",
    "print(\"Test Data r2 Score\",test_r2)\n",
    "print(\".......................................RMSE................................................\")\n",
    "print(\"Training DataSet RMSE:\",train_RMSE)\n",
    "print(\"Testing Dataset RMSE\",test_RMSE)\n",
    "print(\".......................................Rfe model summary.............................................\\n\\n\")\n",
    "print(rfe_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e21ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot our Actual y points vs their predictions\n",
    "plt.style.use('ggplot')\n",
    "fig,ax = plt.subplots(figsize = (12,6))\n",
    "sns.lineplot(x = y_test.index,y = y_test,label = 'True Value',ax = ax)\n",
    "sns.lineplot(x = y_test.index,y = y_pred_test,label = 'Predicted Value',ax = ax)\n",
    "ax.set_xlabel(\"Test examples index\")\n",
    "ax.set_ylabel(\"True and Predicted Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let observe the important features but eliminate the constant\n",
    "key_features = rfe_model.params.index\n",
    "key_features = key_features[1:]\n",
    "key_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the Linearity of the Model\n",
    "#Lets use the plots of (observed vs predicted values) and (residual vs predicted values)\n",
    "\n",
    "def linearity_test(model,y):\n",
    "    predicted_vals = model.predict()\n",
    "    residuals = model.resid\n",
    "    \n",
    "    sns.set_style(\"darkgrid\")\n",
    "    fig,ax = plt.subplots(1,2, figsize = (15,4))\n",
    "    \n",
    "    sns.regplot(x = predicted_vals,y = y,lowess = True,ax = ax[0],line_kws = {'color':'green'})\n",
    "    ax[0].set_title(\"Predicted vs Observed Values\",fontsize = 16)\n",
    "    ax[0].set_xlabel(\"Predicted Values\",fontsize = 13)\n",
    "    ax[0].set_ylabel(\"True Y values(Observed)\",fontsize = 13)\n",
    "    \n",
    "    sns.regplot(x = predicted_vals,y = residuals,lowess = True,ax = ax[1],line_kws = {'color':'blue'})\n",
    "    ax[1].set_title(\"Predicted Values vs Residuals\",fontsize = 16)\n",
    "    ax[1].set_xlabel(\"Predicted Values\",fontsize = 13)\n",
    "    ax[1].set_ylabel(\"Residuals\",fontsize = 13)\n",
    "    fig.show()\n",
    "    \n",
    "linearity_test(rfe_model,y_train)\n",
    "\n",
    "#Desired outcome is that points are symmetrically distributed around a diagonal line in the (Predicted vs Observed) plot and\n",
    "# distributed a long an Howizontal line in the (Predicted vs Residuals) plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0188dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can observe that the Predicted vs Observed has most values closer to the diagonal\n",
    "# Predicted vs Residuals does not have its values symmetrically distributed along a horizontal line\n",
    "# but rather the values take a fan shaped pattern possibly because of outliers and therefore we cannot confirm LINEARITY.\n",
    "\n",
    "#We can now test our model for homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Homoscedasticity  refer to the constant variance in the error terms against the predictions or against any independent variable\n",
    "#This assumption can be tested by visual inspection of a standardized residual plot by the standardized regression \n",
    "#predicted value. Ideally, when the residuals are evenly scattered around the horizontal line, there is presence \n",
    "#of homoscedasticity; and when the residuals are not evenly scattered around the horizontal line and takes a various \n",
    "#shape like a bowtie, funnel shape, etc., then there is the presence of heteroscedasticity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab6f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.api as sms\n",
    "\n",
    "def homoscedasticity_test(model):\n",
    "    \n",
    "    predicted_values = model.predict()\n",
    "    residuals = model.resid\n",
    "    residuals_standardized = model.get_influence().resid_studentized_internal\n",
    "    \n",
    "    sns.set_style('darkgrid')\n",
    "    \n",
    "    fig,ax = plt.subplots(1,2,figsize = (15,8))\n",
    "    sns.regplot(x = predicted_values,y = residuals,ax = ax[0],lowess = True,line_kws = {'color':'red'})\n",
    "    ax[0].set_xlabel(\"Predicted Values\",fontsize = 13)\n",
    "    ax[0].set_ylabel(\"Residuals\",fontsize = 13)\n",
    "    ax[0].set_title(\"Predicted Values vs Residuals\",fontsize = 16)\n",
    "    \n",
    "    sns.regplot(x = predicted_values,y = np.sqrt(np.abs(residuals_standardized)),ax = ax[1],lowess = True,line_kws = {'color' : 'red'})\n",
    "    ax[1].set_xlabel(\"Predicted Values\",fontsize = 13)\n",
    "    ax[1].set_ylabel(\"Standardized residuals\",fontsize = 13)\n",
    "    ax[1].set_title(\"Predicted Values vs Standardized Residuals\",fontsize = 16)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a06883",
   "metadata": {},
   "outputs": [],
   "source": [
    "homoscedasticity_test(rfe_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above graphs show that Homoscedasticity cannot be confirmed and again probably because of outliers\n",
    "# We now test for the normality of residuals.If this assumption is violated,it causes problems with calculating confidence\n",
    "# intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7de88d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def normality_of_residuals_test(model):\n",
    "    \n",
    "    sm.ProbPlot(model.resid).qqplot(line = 's')\n",
    "    plt.title(\"Q-Q Plot\")\n",
    "    \n",
    "    jb = stats.jarque_bera(model.resid)\n",
    "    sw = stats.shapiro(model.resid)\n",
    "    ad = stats.anderson(model.resid, dist='norm')\n",
    "    ks = stats.kstest(model.resid, 'norm')\n",
    "    \n",
    "    print(f'Jarque-Bera test ---- statistic: {jb[0]:.4f}, p-value: {jb[1]}')\n",
    "    print(f'Shapiro-Wilk test ---- statistic: {sw[0]:.4f}, p-value: {sw[1]:.4f}')\n",
    "    print(f'Kolmogorov-Smirnov test ---- statistic: {ks.statistic:.4f}, p-value: {ks.pvalue:.4f}')\n",
    "    print(f'Anderson-Darling test ---- statistic: {ad.statistic:.4f}, 5% critical value: {ad.critical_values[2]:.4f}')\n",
    "    print('If the returned Anderson Draling statistic is larger than the critical value, then for the 5% significance level, the null hypothesis that the data come from the Normal distribution should be rejected. ')\n",
    "    \n",
    "normality_of_residuals_test(rfe_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Q Plot shows deviation from normal distirbution esp at tails and P-value in first 3 normality tests<0.05 and \n",
    "#Anderson-Darling statistic>AD critical value, thus null hypothesis that errors have normal dist is rejected\n",
    "# Let us now identify outliers by ploting standardized residuals vs Leverage and cook's distance and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection(model,top_influencing_obs_count):\n",
    "    influence = model.get_influence()\n",
    "\n",
    "#leverage (hat values)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "\n",
    "#When cases are outside of the Cook’s distance (meaning they have high Cook’s distance scores), \n",
    "#the cases are influential to the regression results. The regression results will be altered if we exclude those cases.\n",
    "    cooks_d = influence.cooks_distance\n",
    "\n",
    "#standardized residuals= (Residual/STD of Residuals)\n",
    "    standardized_residuals = influence.resid_studentized_internal\n",
    "\n",
    "#studentized residuals\n",
    "    studentized_residuals = influence.resid_studentized_external \n",
    "\n",
    "    fig = plt.figure(figsize = (15,8))\n",
    "    plt.scatter(leverage,standardized_residuals,alpha = 0.5)\n",
    "    sns.regplot(leverage,standardized_residuals,scatter = False,ci = False,lowess = True,\n",
    "               line_kws = {'color':'red',\"lw\":1,\"alpha\":0.8})\n",
    "    fig.axes[0].set_xlim(0,max(leverage) + 0.01)\n",
    "    fig.axes[0].set_ylim(-10,6)\n",
    "    fig.axes[0].set_title(\"Standardized Residuals vs Leverage\",fontsize = 16)\n",
    "    fig.axes[0].set_xlabel(\"Leverage\",fontsize = 13)\n",
    "    fig.axes[0].set_ylabel(\"Standardized Residuals\",fontsize = 13);\n",
    "    \n",
    "    leverage_top_n_obs = np.flip(np.argsort(cooks_d[0]), 0)[:top_influencing_obs_count]\n",
    "    \n",
    "    for i  in leverage_top_n_obs:\n",
    "        fig.axes[0].annotate(i,xy = (leverage[i],studentized_residuals[i]))\n",
    "        \n",
    "    def graph(formula,x_range,label = None):\n",
    "        x = x_range\n",
    "        y = formula(x)\n",
    "        plt.plot(x,y,label = label,lw = 1,ls = '--',color = 'red')\n",
    "        \n",
    "    p = len(rfe_model.params)\n",
    "        \n",
    "    graph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), np.linspace(0.001, max(leverage), 50),'Cook\\'s distance')#cookd= 0.5 line\n",
    "    plt.legend(loc='upper right'); \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a41726",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detection(model = rfe_model,top_influencing_obs_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c85d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We find that 2 observations 16 & 91 are outside cook's distance lines, thus they have to removed . \n",
    "#Also observation 24 is close to 0.5 cook's line and has a high standardized residual, also removing it \n",
    "\n",
    "X_train_no_outliers=X_train.drop(index=[16,24,91])\n",
    "y_train_no_outliers=y_train.drop(index=[16,24,91])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edf133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us now Rebuild the model using RFE nad K Fold Cross Validation\n",
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sm\n",
    "\n",
    "train_r2 = []\n",
    "test_r2 = []\n",
    "train_RMSE = []\n",
    "test_RMSE = []\n",
    "\n",
    "for n_features in range(4,31):\n",
    "    lg = LinearRegression()\n",
    "    \n",
    "    rfe = RFE(estimator = lg,n_features_to_select = n_features)\n",
    "    \n",
    "    rfe.fit(X_train_no_outliers,y_train_no_outliers)\n",
    "    \n",
    "    selected_features = X_train_no_outliers.columns[rfe.support_]\n",
    "    \n",
    "    X_train_rfe = X_train_no_outliers[selected_features]\n",
    "    X_test_rfe = X_test[selected_features]\n",
    "    \n",
    "    X_train_rfe = sm.add_constant(X_train_rfe,has_constant = 'add')\n",
    "    X_test_rfe = sm.add_constant(X_test_rfe,has_constant = 'add')\n",
    "    \n",
    "    rfe_model = sm.OLS(y_train_no_outliers,X_train_rfe).fit()\n",
    "    \n",
    "    y_pred_train = rfe_model.predict(X_train_rfe)\n",
    "    y_pred_test = rfe_model.predict(X_test_rfe)\n",
    "    \n",
    "    #Metrics\n",
    "    train_r2.append(rfe_model.rsquared)\n",
    "    test_r2.append(r2_score(y_test,y_pred_test))\n",
    "    \n",
    "    error_test=y_pred_test-y_test\n",
    "    error_train=y_pred_train-y_train_no_outliers\n",
    "    \n",
    "    test_RMSE.append(((error_test**2).mean())**0.5)\n",
    "    train_RMSE.append(((error_train**2).mean())**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting R2 Score Against the number of features\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(range(4,31),train_r2,'r',label = 'R2 Train')\n",
    "plt.plot(range(4,31),test_r2,'b',label = 'R2 Test')\n",
    "plt.xticks(range(2,35))\n",
    "plt.xlabel(\"No. of Features\")\n",
    "plt.ylabel(\"R2 Scores\")\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72942db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Mean Squared error\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(range(4,31),train_MSE,'r',label = 'RMSE Train')\n",
    "plt.plot(range(4,31),test_MSE,'b',label = 'RMSE Test')\n",
    "plt.xticks(range(2,35))\n",
    "plt.xlabel(\"No. of Features\")\n",
    "plt.ylabel(\"Root Mean Squared Error\")\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Model appears to be highly overfitting so we can't select the optimal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can observe by what percentage the RMSE of the test set in higher compared to train set RMSE\n",
    "RMSE_test_dividedby_train = [i / j for i, j in zip(test_RMSE, train_RMSE)]\n",
    "RMSE_test_dividedby_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff37f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can cleary see that the model is overfitting.test RMSE is 32% higher than train RMSE for 4 features and 45% for 5 features.\n",
    "# We can overcome this by trying K Fold cross validation to obtain optimal features\n",
    "\n",
    "#Let us first prepare our data for K fold\n",
    "X_cv = X.drop(index = [16,24,91])\n",
    "y_cv = y.drop(index = [16,24,91])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21772434",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv.reset_index(drop = True,inplace = True)\n",
    "y_cv.reset_index(drop = True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_cv.shape,y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4179ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perfom K-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits = 5,shuffle = True,random_state = 42)\n",
    "\n",
    "for n_features in range(5,31):\n",
    "    train_r2 = []\n",
    "    test_r2 = []\n",
    "    train_RMSE = []\n",
    "    test_RMSE = []\n",
    "    \n",
    "    for train,test in kfold.split(X_cv):\n",
    "        lg = LinearRegression()\n",
    "        \n",
    "        rfe = RFE(estimator = lg,n_features_to_select = n_features)\n",
    "        \n",
    "        rfe.fit(X_cv.loc[train],y_cv[train])\n",
    "        \n",
    "        selected_columns = X_cv.columns[rfe.support_]\n",
    "        \n",
    "        X_cv_rfe = X_cv[selected_columns]\n",
    "        \n",
    "        X_cv_rfe = sm.add_constant(X_cv_rfe,has_constant ='add')\n",
    "        \n",
    "        rfe_model = sm.OLS(y_cv[train],X_cv_rfe.loc[train]).fit()\n",
    "        \n",
    "        y_pred_train = rfe_model.predict(X_cv_rfe.loc[train])\n",
    "        y_pred_test = rfe_model.predict(X_cv_rfe.loc[test])\n",
    "        \n",
    "        #R-square\n",
    "        train_r2.append(r2_score(y_pred_train , y_cv[train]))\n",
    "        test_r2.append(r2_score(y_pred_test , y_cv[test]))\n",
    "        \n",
    "        #Error\n",
    "        error_train = y_pred_train - y_cv[train]\n",
    "        error_test = y_pred_test - y_cv[test]\n",
    "        rmse_train=((error_train**2).mean())**0.5\n",
    "        rmse_test=((error_test**2).mean())**0.5\n",
    "        \n",
    "        train_RMSE.append(rmse_train)\n",
    "        test_RMSE.append(rmse_test)\n",
    "        \n",
    "        test_times_train=np.mean(test_RMSE)/np.mean(train_RMSE)\n",
    "        # generate report\n",
    "        print('n_features:{:1} |train_R2:{:2} |test_R2:{:3} |mean(rmse_train):{:4} |mean(rmse_test):{:5} |RMSE(test/train):{}'.\n",
    "          format(n_features, round(np.mean(train_r2),4), round(np.mean(test_r2),4),\n",
    "                 round(np.mean(train_RMSE),0),\n",
    "                 round(np.mean(test_RMSE),0),round(test_times_train,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1718d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beyond 6 features,there is massive overfitting. To deal with this I ran the model with 5 and 6 features and found multicollinearity\n",
    "# Features: carwidth and curbweight are highly correlated and latter has a VIF of 7 plus. \n",
    "#So when n=6, I remove 'curbweight' leaving us with n=5.\n",
    "#So Model with 6 Features\n",
    "import statsmodels.api as sm\n",
    "\n",
    "lg = LinearRegression()\n",
    "\n",
    "rfe = RFE(estimator = lg,n_features_to_select = 6)\n",
    "\n",
    "rfe.fit(X_cv, y_cv)\n",
    "\n",
    "selected_columns = X_cv.columns[rfe.support_]\n",
    "\n",
    "X_cv_rfe = X_cv[selected_columns]\n",
    "\n",
    "X_cv_rfe = sm.add_constant(X_cv_rfe,has_constant = 'add')\n",
    "\n",
    "rfe_model = sm.OLS(y_cv,X_cv_rfe).fit()\n",
    "\n",
    "rfe_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE now test multicollinearity-(none of the features should be highly correlated to other)\n",
    "\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "\n",
    "def variance_inflation_factors(X_df):\n",
    "    X_df = add_constant(X_df)\n",
    "    vifs = pd.Series(\n",
    "        [1 / (1. - OLS(X_df[col].values, \n",
    "                       X_df.loc[:, X_df.columns != col].values).fit().rsquared) \n",
    "         for col in X_df],\n",
    "        index=X_df.columns,\n",
    "        name='VIF'\n",
    "    )\n",
    "    return vifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_inflation_factors(X_cv_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9041b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#curbweight VIF=7.697143, highly correlated with carwidth (0.87 corr coeff) so removing curbweight\n",
    "X_final =X_cv_rfe.loc[:,X_cv_rfe.columns !='curbweight']\n",
    "X_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02247a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Linear Model Again\n",
    "rfe_model =sm.OLS(y_cv,X_final).fit()\n",
    "\n",
    "y_predictions=rfe_model.predict(X_final)\n",
    "\n",
    "\n",
    "#Standard error/RMSE\n",
    "error=y_predictions-y_cv\n",
    "\n",
    "print('RMSE is: {}'.format(((error**2).mean())**0.5))\n",
    "\n",
    "print(rfe_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(15,8))\n",
    "sns.lineplot(x=y_cv.index,y=y_cv,label='Actuals',color='blue',ax=ax)\n",
    "sns.lineplot(x=y_cv.index,y=y_predictions,label='Predictions',color='red',ax=ax)\n",
    "ax.set_title('Price: Actuals vs Predictions', fontsize=16)\n",
    "ax.set_ylabel('Car Price',fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2061ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now check for linearity for our new model\n",
    "\n",
    "linearity_test(rfe_model,y_cv)\n",
    "\n",
    "#Residuals more or less evenly scattered vs predicted values-looks fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f31085",
   "metadata": {},
   "outputs": [],
   "source": [
    "homoscedasticity_test(rfe_model)  #both graphs show evenly spread residuals so homoscedasticity is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a348e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detection(model = rfe_model,top_influencing_obs_count=10) #no outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_of_residuals_test(rfe_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4856d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The final model meets all the assumptions including no mutlicollinearity & no outliers and has an R-Square of 86%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
